# -*- coding: utf-8 -*-
"""Adaptive Applications.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CVJp6b03eeVBavmpEU4AI3Sy-3Rk0H6I
"""
print("In Python File")


import numpy as np
import pandas as pd
import sys

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
import math
import csv
from nltk.tokenize import word_tokenize

import nltk
nltk.download('words')
from nltk.corpus import stopwords
nltk.download('stopwords')
import nltk
nltk.download('punkt')

from nltk.corpus import words
word_list = words.words()
stop_words = stopwords.words()

#len(word_list)

frequency_list = pd.read_csv(sys.argv[1] + "\\pyFileDir\\unigram_freq.csv")
print("In Python File")
frequency_list.loc[:,'word'] = frequency_list.loc[:,'word'].str.lower()

for i in range(len(frequency_list["word"])):
    if frequency_list.loc[i,'word'] in stop_words:
        frequency_list.drop(labels=i,axis=0,inplace=True)

frequency_list.reset_index(drop = True, inplace=True)

normalized = (frequency_list[["count"]] - frequency_list[["count"]].mean())/frequency_list[["count"]].std()
frequency_list["count_nomalized"] = normalized
frequency_list.drop(labels="count",inplace=True,axis=1)
frequency_list

frequency_list['count_nomalized'].describe()

frequency_list['groups'] = pd.qcut(frequency_list['count_nomalized'], q = 25, 
                                   labels=[25,24,23,22,21,20,19,18,17,16,15,14,
                                           13,12,11,10,9,8,7,6,5,4,3,2,1])          #lebel 1 contains easiest words
#len(list(frequency_list.loc[frequency_list['groups']==1,'word']))
#list(frequency_list.loc[frequency_list['groups']==1,'word'])
frequency_list = frequency_list.astype({'groups':int})
frequency_list.head()

frequency_list[frequency_list['word']=='predictive']

doc = open(sys.argv[1] + "\\upload-dir\\hello.txt",'r',encoding='utf-8')
text = doc.read()
doc.close()
text

import string

punctuations = list(string.punctuation)
remove_words = stop_words + punctuations
remove_words = remove_words + list('’“‚—–”‘)..”%.')

#Converting text file into tokens

from nltk.tokenize import wordpunct_tokenize
tokens = wordpunct_tokenize(text)
tokens = list(map(str.lower,tokens))
#print(tokens)
#tokens

#removing punctuations and stop words from the document
#also lemmatising words

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

clean_words = []

for word in tokens:
    if word not in remove_words:
        clean_words.append(word)
print(clean_words)

from collections import Counter
Counter(clean_words)
words_dataframe = pd.DataFrame(list(Counter(clean_words).items()),columns=['words','Frequency'])
words_dataframe.head()

type(frequency_list.loc[0,'groups'])

#the group of frequency_list for a word in frequency_list['word'] and in words_dataframe['words']
#frequency_list.loc(frequency_list['word'] == words_dataframe.loc[0,'words'], 'group')
int(frequency_list.loc[frequency_list['word'] == words_dataframe.loc[0,'words'],'groups'])

words_dataframe['groups'] = 0
words_dataframe.loc[30:34,'words']

"1" in list(frequency_list['word'])

for i in range(len(words_dataframe)):
    #print(words_dataframe.loc[i,'words'])
    try:
        group = int(frequency_list.loc[frequency_list['word'] == words_dataframe.loc[i,'words'],'groups'])
        words_dataframe.loc[i,'groups'] = group
    except TypeError:
        pass
    #print(i)
    
words_dataframe.head(20)

sum(words_dataframe['Frequency'])

sum(words_dataframe['Frequency']*words_dataframe['groups'])

#Calculating vocabulary score
#Vocablary score or average group score = sum(frequency*groups)/sum (frequency)

vocabulary_score = sum(words_dataframe['Frequency']*words_dataframe['groups'])/sum(words_dataframe['Frequency'])
round(vocabulary_score) #so unknown words = all words of group greater then 2

frequency_list.loc[frequency_list['groups']>vocabulary_score,'word']
#frequency_list['groups']>vocabulary_score

unknown_words = list(frequency_list.loc[frequency_list['groups']>vocabulary_score,'word'])






with open(sys.argv[1] + '\\pyFileDir\\unknown_words.txt','w') as f:
  f.write('\n'.join(unknown_words))

print("File is written")
