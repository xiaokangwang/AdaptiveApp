# -*- coding: utf-8 -*-
"""Adaptive clean code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jwwynpiZA0mW-ArHy2v-GsGkgwXOMt-C
"""

import numpy as np
import pandas as pd
import math
import sys
import ssl
import os
from nltk.tokenize import word_tokenize

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

import nltk
nltk.download('words')
from nltk.corpus import stopwords
nltk.download('stopwords')
import nltk
nltk.download('punkt')
import nltk
nltk.download('wordnet')

from nltk.corpus import words


word_list = words.words()
stop_words = stopwords.words()

frequency_list = pd.read_csv(os.path.join(sys.argv[1],"pyFileDir","frequency_list.csv"))
frequency_list.drop(frequency_list.columns[0],axis=1,inplace=True)

normalized = (frequency_list[["count"]] - frequency_list[["count"]].mean())/frequency_list[["count"]].std()
frequency_list["count_nomalized"] = normalized
frequency_list.drop(labels="count",inplace=True,axis=1)
frequency_list['groups'] = pd.qcut(frequency_list['count_nomalized'], q = 21, 
                                   labels=[21,20,19,18,17,16,15,14,
                                           13,12,11,10,9,8,7,6,5,4,3,2,1])          #lebel 1 contains easiest words

frequency_list = frequency_list.astype({'groups':int})

doc = open(os.path.join(sys.argv[1],"upload-dir","hello.txt"),'r',encoding='utf-8')
text = doc.read()
doc.close()

import string

punctuations = list(string.punctuation)
remove_words = stop_words + punctuations
remove_words = remove_words + list('’“‚—–”‘)..”%.')

#Converting text file into tokens

from nltk.tokenize import wordpunct_tokenize
tokens = wordpunct_tokenize(text)
tokens = list(map(str.lower,tokens))

#removing punctuations and stop words from the document
#also lemmatising words

from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

clean_words = []

for word in tokens:
    if word not in remove_words:
        clean_words.append(lemmatizer.lemmatize(word))

from collections import Counter
Counter(clean_words)
words_dataframe = pd.DataFrame(list(Counter(clean_words).items()),columns=['words','Frequency'])
words_dataframe['groups'] = 0
for i in range(len(words_dataframe)):
    try:
        group = int(frequency_list.loc[frequency_list['word'] == words_dataframe.loc[i,'words'],'groups'])
        words_dataframe.loc[i,'groups'] = group
    except TypeError:
        pass

#Calculating vocabulary score
#Vocablary score or average group score = sum(frequency*groups)/sum (frequency)

vocabulary_score = sum(words_dataframe['Frequency']*words_dataframe['groups'])/sum(words_dataframe['Frequency'])
vocabulary_score = math.ceil(vocabulary_score) #so unknown words = all words of group greater than 2

unknown_words = list(frequency_list.loc[frequency_list['groups']>vocabulary_score,'word'])
known_words = list(frequency_list.loc[frequency_list['groups']<=vocabulary_score,'word'])

with open(os.path.join(sys.argv[1],"pyFileDir","unknown_words.txt"),'w') as f:
  f.write('\n'.join(unknown_words))

fileknownwords = []
for w in known_words:
    fileknownwords.append(str(w))

with open(os.path.join(sys.argv[1],"pyFileDir","known_words.txt"),'w') as f:
  f.write('\n'.join(fileknownwords))